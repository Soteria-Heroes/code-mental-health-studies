{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           PID                                          Text_data     Label\n",
      "0  train_pid_1  Waiting for my mind to have a breakdown once t...  moderate\n",
      "1  train_pid_2  My new years resolution : I'm gonna get my ass...  moderate\n",
      "2  train_pid_3  New year : Somone else Feeling like 2020 will ...  moderate\n",
      "3  train_pid_4  My story I guess : Hi, Im from Germany and my ...  moderate\n",
      "4  train_pid_5  Sat in the dark and cried myself going into th...  moderate\n",
      "          Pid                                          text data Class labels\n",
      "0  test_pid_1  Im scared : This is it. I lie to myself every ...     moderate\n",
      "1  test_pid_2  New to this but just wanted to vent : I just f...     moderate\n",
      "2  test_pid_3  I’m sad : It’s kinda always been an issue. I w...     moderate\n",
      "3  test_pid_4  Lonely but not alone. : All of my immediately ...     moderate\n",
      "4  test_pid_5  This year has been trash. : I dont know why I’...     moderate\n",
      "         PID                                          Text data     Label\n",
      "0  dev_pid_1  I enjoyed today, and I still am! Tomorrows dep...  moderate\n",
      "1  dev_pid_2  I sorta tried to kill myself : I had a total b...  moderate\n",
      "2  dev_pid_3  Best suicide method? : I like it quick and eas...  moderate\n",
      "3  dev_pid_4  a story : I remember the time I'd get on my 3D...  moderate\n",
      "4  dev_pid_5  The world only cares about beautiful people : ...  moderate\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(r\"D:\\Soteria_NLP\\Raw_files\\train.tsv\", sep ='\\t' )\n",
    "df_test = pd.read_csv(r\"D:\\Soteria_NLP\\Raw_files\\test.tsv\", sep= '\\t')\n",
    "df_dev = pd.read_csv(r\"D:\\Soteria_NLP\\Raw_files\\dev.tsv\", sep = '\\t')\n",
    "\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(df_dev.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8891 entries, 0 to 8890\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   PID        8891 non-null   object\n",
      " 1   Text_data  8891 non-null   object\n",
      " 2   Label      8891 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 208.5+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3245 entries, 0 to 3244\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Pid           3245 non-null   object\n",
      " 1   text data     3245 non-null   object\n",
      " 2   Class labels  3245 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 76.2+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4496 entries, 0 to 4495\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   PID        4496 non-null   object\n",
      " 1   Text data  4496 non-null   object\n",
      " 2   Label      4496 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 105.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())\n",
    "print(df_test.info())\n",
    "print(df_dev.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moderate          6004\n",
      "not depression    1985\n",
      "severe             902\n",
      "Name: Label, dtype: int64\n",
      "moderate          2169\n",
      "not depression     848\n",
      "severe             228\n",
      "Name: Class labels, dtype: int64\n",
      "moderate          2306\n",
      "not depression    1830\n",
      "severe             360\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['Label'].value_counts())\n",
    "print(df_test['Class labels'].value_counts())\n",
    "print(df_dev['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_train = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXP-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ', str(text))\n",
    "\n",
    "    # Remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "    \n",
    "\n",
    "    # Substitute multiple spaces with single space\n",
    "    text = re.sub(r'\\s+|\\d+', ' ', text, flags=re.I)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Lemmatization\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if not word in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "df_train['Text_data'] = df_train['Text_data'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_train = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train['Text_data']\n",
    "y= df_train['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moderate          4803\n",
       "not depression    1588\n",
       "severe             721\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "#### Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word): \n",
    "        for lemma in syn.lemmas(): \n",
    "            synonyms.add(lemma.name())\n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    return list(synonyms)\n",
    "\n",
    "def synonym_replacement(words, n=5):\n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "df_moderate = df_train[df_train['Label'] == 'moderate']\n",
    "df_not_depression = df_train[df_train['Label'] == 'not depression']\n",
    "df_severe = df_train[df_train['Label'] == 'severe']\n",
    "\n",
    "df_not_depression_over = df_not_depression.sample(int(len(df_not_depression)*0.5), replace=True, random_state=42)\n",
    "df_not_depression_over['Text_data'] = df_not_depression_over['Text_data'].apply(synonym_replacement)\n",
    "\n",
    "df_severe_over = df_severe.sample(len(df_severe)*2, replace=True, random_state=42)\n",
    "df_severe_over['Text_data'] = df_severe_over['Text_data'].apply(synonym_replacement)\n",
    "\n",
    "df_moderate_under = df_moderate.sample(len(df_not_depression) + int(len(df_not_depression)*0.5), random_state=42)\n",
    "\n",
    "df_train_aug = pd.concat([df_moderate_under, df_not_depression, df_not_depression_over, df_severe, df_severe_over])\n",
    "\n",
    "X_train_aug = df_train_aug['Text_data']\n",
    "y_train_aug = df_train_aug['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moderate          2382\n",
       "not depression    2382\n",
       "severe            2163\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_aug.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 2707\n"
     ]
    }
   ],
   "source": [
    "duplicates = X_train_aug.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "\n",
    "# Print the number of duplicate rows\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Assuming 'X_train_aug' and 'y_train_aug' are your DataFrames\n",
    "duplicates = X_train_aug.duplicated(keep=False)  # Find all duplicate rows\n",
    "duplicate_indices = duplicates[duplicates].index  # Get the indices of duplicate rows\n",
    "\n",
    "# Filter the duplicate rows in 'X_train_aug' and their corresponding rows in 'y_train_aug'\n",
    "duplicate_data = X_train_aug.loc[duplicate_indices]\n",
    "duplicate_labels = y_train_aug.loc[duplicate_indices]\n",
    "\n",
    "# Combine 'X_train_aug' and 'y_train_aug' for duplicate rows into a single DataFrame\n",
    "duplicate_combined = pd.concat([duplicate_data, duplicate_labels], axis=1)\n",
    "\n",
    "# Download the combined DataFrame as a CSV file\n",
    "duplicate_combined.to_csv('duplicate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moderate          4803\n",
       "not depression    1588\n",
       "severe             721\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_scores(label):\n",
    "    if label == 'not depression':\n",
    "        return 0.25\n",
    "    elif label == 'moderate':\n",
    "        return 0.5\n",
    "    elif label == 'severe':\n",
    "        return 1.0\n",
    "\n",
    "y_train = y_train.apply(convert_labels_to_scores)\n",
    "y_train_aug = y_train_aug.apply(convert_labels_to_scores)\n",
    "y_val = y_val.apply(convert_labels_to_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.82156917885264 201.0\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(x.split()) for x in X_train]\n",
    "avg_length = np.mean(lengths)\n",
    "percentile_95_length = np.percentile(lengths, 95)\n",
    "print(avg_length, percentile_95_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the maximum number of words in the vocabulary and the maximum length for sequences\n",
    "max_words = 10000\n",
    "max_length = 200\n",
    "\n",
    "# Instantiate the Tokenizer and fit it to the training data\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the texts to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_aug_sequences = tokenizer.texts_to_sequences(X_train_aug)\n",
    "X_val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "# Pad the sequences so they're all the same length\n",
    "X_train_pad = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_train_aug_pad = pad_sequences(X_train_aug_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_sequences, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with X_train_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "223/223 [==============================] - 9s 33ms/step - loss: 0.0438 - val_loss: 0.0400\n",
      "Epoch 2/30\n",
      "223/223 [==============================] - 7s 32ms/step - loss: 0.0381 - val_loss: 0.0394\n",
      "Epoch 3/30\n",
      "223/223 [==============================] - 7s 32ms/step - loss: 0.0375 - val_loss: 0.0395\n",
      "Epoch 4/30\n",
      "223/223 [==============================] - 7s 32ms/step - loss: 0.0367 - val_loss: 0.0412\n",
      "Epoch 5/30\n",
      "223/223 [==============================] - 7s 32ms/step - loss: 0.0365 - val_loss: 0.0401\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model_rnn_X_train = Sequential()\n",
    "model_rnn_X_train.add(Embedding(max_words, 32, input_length=max_length))\n",
    "model_rnn_X_train.add(SimpleRNN(32, return_sequences=False))\n",
    "model_rnn_X_train.add(Dense(1))\n",
    "\n",
    "model_rnn_X_train.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_rnn_X_train.fit(X_train_pad, y_train, epochs=30, validation_data=(X_val_pad, y_val),\n",
    "                                callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 7ms/step\n",
      "Mean Absolute Error: 0.11742201002713601\n",
      "Mean Squared Error: 0.039425625282370834\n",
      "Root Mean Squared Error: 0.19855887107447714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "# Get predicted values\n",
    "y_val_predicted = model_rnn_X_train.predict(X_val_pad)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_val, y_val_predicted)\n",
    "print('Mean Absolute Error:', mae)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_val, y_val_predicted)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with X_train_aug_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "217/217 [==============================] - 8s 33ms/step - loss: 0.0987 - val_loss: 0.0544\n",
      "Epoch 2/30\n",
      "217/217 [==============================] - 7s 33ms/step - loss: 0.0903 - val_loss: 0.0447\n",
      "Epoch 3/30\n",
      "217/217 [==============================] - 7s 32ms/step - loss: 0.0870 - val_loss: 0.0514\n",
      "Epoch 4/30\n",
      "217/217 [==============================] - 7s 32ms/step - loss: 0.0793 - val_loss: 0.0491\n",
      "Epoch 5/30\n",
      "217/217 [==============================] - 7s 34ms/step - loss: 0.0681 - val_loss: 0.0473\n"
     ]
    }
   ],
   "source": [
    "model_rnn_X_train_aug = Sequential()\n",
    "model_rnn_X_train_aug.add(Embedding(max_words, 32, input_length=max_length))\n",
    "model_rnn_X_train_aug.add(SimpleRNN(32, return_sequences=False))\n",
    "model_rnn_X_train_aug.add(Dense(1))\n",
    "\n",
    "model_rnn_X_train_aug.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_rnn_X_train_aug.fit(X_train_aug_pad, y_train_aug, epochs=30, validation_data=(X_val_pad, y_val),\n",
    "                                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 8ms/step\n",
      "Mean Absolute Error: 0.16204104187650717\n",
      "Mean Squared Error: 0.044674313636210515\n",
      "Root Mean Squared Error: 0.2113629902234791\n"
     ]
    }
   ],
   "source": [
    "# Get predicted values\n",
    "y_val_predicted = model_rnn_X_train_aug.predict(X_val_pad)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_val, y_val_predicted)\n",
    "print('Mean Absolute Error:', mae)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_val, y_val_predicted)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "223/223 [==============================] - 15s 60ms/step - loss: 0.0498 - val_loss: 0.0393\n",
      "Epoch 2/30\n",
      "223/223 [==============================] - 13s 58ms/step - loss: 0.0390 - val_loss: 0.0391\n",
      "Epoch 3/30\n",
      "223/223 [==============================] - 13s 58ms/step - loss: 0.0381 - val_loss: 0.0391\n",
      "Epoch 4/30\n",
      "223/223 [==============================] - 13s 57ms/step - loss: 0.0378 - val_loss: 0.0413\n",
      "Epoch 5/30\n",
      "223/223 [==============================] - 13s 57ms/step - loss: 0.0375 - val_loss: 0.0390\n",
      "Epoch 6/30\n",
      "223/223 [==============================] - 13s 57ms/step - loss: 0.0378 - val_loss: 0.0396\n",
      "Epoch 7/30\n",
      "223/223 [==============================] - 13s 57ms/step - loss: 0.0372 - val_loss: 0.0399\n",
      "Epoch 8/30\n",
      "223/223 [==============================] - 13s 57ms/step - loss: 0.0371 - val_loss: 0.0396\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "model_lstm_X_train = Sequential()\n",
    "model_lstm_X_train.add(Embedding(max_words, 32, input_length=max_length))\n",
    "model_lstm_X_train.add(LSTM(32, return_sequences=False))\n",
    "model_lstm_X_train.add(Dense(1))\n",
    "\n",
    "model_lstm_X_train.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_lstm_X_train.fit(X_train_pad, y_train_scores, epochs=30, validation_data=(X_val_pad, y_val_scores), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 17ms/step\n",
      "Mean Absolute Error: 0.10778830218442442\n",
      "Mean Squared Error: 0.039004328921241116\n",
      "Root Mean Squared Error: 0.19749513644958733\n"
     ]
    }
   ],
   "source": [
    "# Get predicted values\n",
    "y_val_predicted = model_lstm_X_train.predict(X_val_pad)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "mae = mean_absolute_error(y_val, y_val_predicted)\n",
    "print('Mean Absolute Error:', mae)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_val, y_val_predicted)\n",
    "print('Mean Squared Error:', mse)\n",
    "\n",
    "# Calculate Root Mean Squared Error\n",
    "rmse = np.sqrt(mse)\n",
    "print('Root Mean Squared Error:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 [==============================] - 26s 200ms/step - loss: 0.0505 - val_loss: 0.0393\n",
      "Epoch 2/30\n",
      "112/112 [==============================] - 22s 194ms/step - loss: 0.0392 - val_loss: 0.0388\n",
      "Epoch 3/30\n",
      "112/112 [==============================] - 22s 199ms/step - loss: 0.0389 - val_loss: 0.0399\n",
      "Epoch 4/30\n",
      "112/112 [==============================] - 22s 197ms/step - loss: 0.0377 - val_loss: 0.0386\n",
      "Epoch 5/30\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 0.0376 - val_loss: 0.0389\n",
      "Epoch 6/30\n",
      "112/112 [==============================] - 22s 200ms/step - loss: 0.0372 - val_loss: 0.0409\n",
      "Epoch 7/30\n",
      "112/112 [==============================] - 22s 199ms/step - loss: 0.0374 - val_loss: 0.0388\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model_lstm_X_train = Sequential()\n",
    "model_lstm_X_train.add(Embedding(max_words, 64, input_length=max_length))  # Increased embedding dimension\n",
    "model_lstm_X_train.add(LSTM(64, return_sequences=True))  # Added more units and returning sequences\n",
    "model_lstm_X_train.add(Dropout(0.2))  # Added dropout\n",
    "model_lstm_X_train.add(LSTM(32, return_sequences=False))  # Added another LSTM layer\n",
    "model_lstm_X_train.add(Dense(1))\n",
    "\n",
    "model_lstm_X_train.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')  # Tuned learning rate\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model_lstm_X_train.fit(X_train_pad, y_train_scores, epochs=30, validation_data=(X_val_pad, y_val_scores), batch_size=64, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
